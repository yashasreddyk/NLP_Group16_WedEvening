{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping all the nearly 18000 webpages of UTS."
      ],
      "metadata": {
        "id": "pIIk7-0hAQWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P6oqaMaAIgA"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q beautifulsoup4 requests\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# STEP 1: Get all URLs from UTS sitemap\n",
        "def get_urls_from_sitemap(sitemap_url):\n",
        "    response = requests.get(sitemap_url)\n",
        "    soup = BeautifulSoup(response.content, \"xml\")\n",
        "    return [loc.text for loc in soup.find_all(\"loc\")]\n",
        "\n",
        "# STEP 2: Scrape and save each batch\n",
        "def scrape_and_save_batch(urls, batch_num, batch_size=1000, output_dir=\"/content\"):\n",
        "    docs = []\n",
        "    start = batch_num * batch_size\n",
        "    end = min(start + batch_size, len(urls))\n",
        "\n",
        "    for i, url in enumerate(urls[start:end]):\n",
        "        try:\n",
        "            res = requests.get(url, timeout=5)\n",
        "            soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "                tag.decompose()\n",
        "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "            if len(text) > 100:\n",
        "                docs.append(Document(page_content=text, metadata={\"source\": url}))\n",
        "            print(f\"[{start+i+1}] ‚úÖ {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{start+i+1}] ‚ùå {url} ({e})\")\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    # Save batch\n",
        "    batch_file = os.path.join(output_dir, f\"web_docs_batch_{batch_num}.pkl\")\n",
        "    with open(batch_file, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    print(f\"‚úÖ Saved batch {batch_num} ({len(docs)} docs) to {batch_file}\")\n",
        "\n",
        "# STEP 3: Orchestrate scraping with resume support\n",
        "sitemap_url = \"https://www.uts.edu.au/sitemap.xml\"\n",
        "all_urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "max_pages = 18000\n",
        "batch_size = 1000\n",
        "output_dir = \"/content\"\n",
        "num_batches = (min(len(all_urls), max_pages) + batch_size - 1) // batch_size\n",
        "\n",
        "print(f\"üåê Total URLs: {len(all_urls)} | Max pages: {max_pages} | Total batches: {num_batches}\")\n",
        "\n",
        "for batch_num in range(num_batches):\n",
        "    batch_file = os.path.join(output_dir, f\"web_docs_batch_{batch_num}.pkl\")\n",
        "    if os.path.exists(batch_file):\n",
        "        print(f\"‚è© Batch {batch_num} already exists. Skipping.\")\n",
        "        continue\n",
        "    scrape_and_save_batch(all_urls, batch_num, batch_size, output_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining the webpage document batches into a single file."
      ],
      "metadata": {
        "id": "LmSmGiY2AWWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, glob\n",
        "\n",
        "# Load all batch files\n",
        "batch_files = sorted(glob.glob(\"/content/web_docs_batch_*.pkl\"))\n",
        "all_docs = []\n",
        "\n",
        "for file in batch_files:\n",
        "    with open(file, \"rb\") as f:\n",
        "        all_docs.extend(pickle.load(f))\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(all_docs)} documents from {len(batch_files)} batches.\")\n",
        "\n",
        "# Save to a single combined .pkl file\n",
        "with open(\"/content/web_docs_combined.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_docs, f)\n",
        "\n",
        "print(\"üíæ Combined documents saved to /content/web_docs_combined.pkl\")\n"
      ],
      "metadata": {
        "id": "aFFhWS3RAY1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building vectors and training the LLM models"
      ],
      "metadata": {
        "id": "DlLB7mQwAayz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install packages\n",
        "!pip install -q langchain-community langchain faiss-cpu sentence-transformers pdfplumber gradio transformers accelerate huggingface_hub\n",
        "\n",
        "# STEP 2: Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 3: Imports\n",
        "import os, pickle\n",
        "import pdfplumber\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "# STEP 4: Load PDFs\n",
        "pdf_folder_path = \"/content/drive/MyDrive/UTS_Chatbot/training_pdf\"\n",
        "\n",
        "def load_all_pdfs_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            full_path = os.path.join(folder_path, filename)\n",
        "            with pdfplumber.open(full_path) as pdf:\n",
        "                text = \"\"\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                documents.append(Document(page_content=text, metadata={\"source\": filename}))\n",
        "    return documents\n",
        "\n",
        "pdf_docs = load_all_pdfs_from_folder(pdf_folder_path)\n",
        "\n",
        "# STEP 5: Load web docs\n",
        "with open(\"/content/web_docs_combined.pkl\", \"rb\") as f:\n",
        "    web_docs = pickle.load(f)\n",
        "\n",
        "# STEP 6: Combine & Split\n",
        "all_docs = pdf_docs + web_docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)\n",
        "documents = splitter.split_documents(all_docs)\n",
        "\n",
        "# Load FAISS vectorstore from disk\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "faiss_path = \"/content/drive/MyDrive/UTS_Chatbot/faiss_index\"\n",
        "vectorstore = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# STEP 7: Embedding & FAISS (Training from scratch)\n",
        "#embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "#vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# STEP 8: Load all models\n",
        "hf_token = \"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\"\n",
        "\n",
        "model_ids = {\n",
        "    \"LLaMA 3.2 3B\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    \"Phi Mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"Gemma 2B\": \"google/gemma-2b-it\",\n",
        "    \"Tiny LLaMA\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "}\n",
        "\n",
        "pipelines = {}\n",
        "\n",
        "for name, model_id in model_ids.items():\n",
        "    print(f\"üîÑ Loading: {name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        token=hf_token,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "    pipelines[name] = pipe\n",
        "\n",
        "# STEP 9: Prompt & chatbot function\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"\n",
        "You are an expert guide for international students at UTS.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_chatbot_func(model_name):\n",
        "    pipe = pipelines[model_name]\n",
        "\n",
        "    def chatbot(query):\n",
        "        docs = retriever.get_relevant_documents(query)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])[:1000]\n",
        "        prompt = format_prompt(context, query)\n",
        "        result = pipe(prompt)[0][\"generated_text\"]\n",
        "        return result.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    return chatbot\n",
        "\n",
        "# STEP 10: Gradio UI with tabs for each model\n",
        "tabs = []\n",
        "for model_name in model_ids:\n",
        "    with gr.Tab(model_name):\n",
        "        tabs.append(gr.Interface(\n",
        "            fn=create_chatbot_func(model_name),\n",
        "            inputs=\"text\",\n",
        "            outputs=\"text\",\n",
        "            title=f\"ü§ñ UTS Student assistant ({model_name})\",\n",
        "            description=\"Ask anything about life and study at UTS\"\n",
        "        ))\n",
        "\n",
        "gr.TabbedInterface(tabs, tab_names=list(model_ids.keys())).launch()\n"
      ],
      "metadata": {
        "id": "MZNasGoSApUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}